# StudyChat Cognitive Presence Analysis Pipeline

A comprehensive pipeline for analyzing cognitive presence in educational conversations using LLM-based classification and CP-Bench metrics.

## ğŸ—ï¸ Project Structure

```
COI/
â”œâ”€â”€ ğŸ“ src/                    # Core pipeline modules
â”‚   â”œâ”€â”€ data/                  # Data loading and processing
â”‚   â”œâ”€â”€ classification/        # LLM classification components
â”‚   â”œâ”€â”€ evaluation/           # Metrics and evaluation
â”‚   â”œâ”€â”€ visualization/        # Plotting and analysis
â”‚   â””â”€â”€ utils/               # Utilities and helpers
â”œâ”€â”€ ğŸ“ config/               # Configuration files
â”œâ”€â”€ ğŸ“ data/                 # Data storage
â”‚   â”œâ”€â”€ raw/                 # Original datasets
â”‚   â””â”€â”€ processed/           # Processed data
â”œâ”€â”€ ğŸ“ results/              # Pipeline outputs
â”œâ”€â”€ ğŸ“ analysis/             # Analysis and visualizations
â”œâ”€â”€ ğŸ“ notebooks/            # Jupyter notebooks
â”œâ”€â”€ ğŸ“ scripts/              # Standalone scripts
â””â”€â”€ ğŸ“ docs/                 # Documentation
```

## ğŸš€ Quick Start

### 1. Installation
```bash
# Clone the repository
git clone <repository-url>
cd COI

# Install dependencies
pip install -r requirements.txt

# Set your OpenAI API key
export OPENAI_API_KEY="your-api-key-here"
```

### 2. Run the Complete Pipeline
```bash
# Run the entire pipeline on a subset
python scripts/run_pipeline.py --subset 100

# Run on full dataset
python scripts/run_pipeline.py --full

# Run with custom configuration
python scripts/run_pipeline.py --config config/custom_config.json
```

### 3. Generate Analysis
```bash
# Create comprehensive analysis and visualizations
python scripts/generate_analysis.py

# View results
open analysis/overview_dashboard.png
```

## ğŸ“‹ Pipeline Overview

The pipeline consists of 4 main stages:

### Stage 1: Data Loading & Preprocessing
- Load StudyChat dataset from HuggingFace
- Filter and clean messages
- Create conversation threads
- Save processed data

### Stage 2: LLM Classification
- Classify messages using GPT-4
- Apply post-processing rules
- Generate confidence scores
- Save raw and final classifications

### Stage 3: Metrics Computation
- Calculate CP-Bench metrics (SWS, PC, RA, CPI)
- Generate bootstrap confidence intervals
- Compute thread-level statistics
- Analyze role-based patterns

### Stage 4: Analysis & Visualization
- Create comprehensive visualizations
- Generate detailed reports
- Perform bias analysis
- Save reproducibility artifacts

## ğŸ› ï¸ Usage Examples

### Basic Usage
```python
from src.pipeline import CognitivePresencePipeline

# Initialize pipeline
pipeline = CognitivePresencePipeline()

# Run complete analysis
results = pipeline.run(subset_size=100)

# Access results
print(f"CPI: {results.cpi:.3f}")
print(f"Stage distribution: {results.stage_distribution}")
```

### Custom Configuration
```python
from src.config import Config

# Create custom config
config = Config(
    model_name="gpt-4",
    max_tokens=100,
    temperature=0.1,
    pilot_limit=500
)

# Run with custom config
pipeline = CognitivePresencePipeline(config)
results = pipeline.run()
```

### Individual Components
```python
from src.data import StudyChatLoader
from src.classification import LLMClassifier
from src.evaluation import CPBenchMetrics

# Load data
loader = StudyChatLoader()
messages = loader.load(subset_size=100)

# Classify messages
classifier = LLMClassifier()
classifications = classifier.classify(messages)

# Compute metrics
metrics = CPBenchMetrics()
results = metrics.compute(classifications)
```

## ğŸ“Š Output Files

### Data Files
- `data/processed/messages.parquet` - Processed messages
- `data/processed/classifications_raw.parquet` - Raw LLM outputs
- `data/processed/classifications_final.parquet` - Post-processed classifications

### Results Files
- `results/thread_metrics.csv` - Per-thread CP-Bench metrics
- `results/aggregate_metrics.csv` - Bootstrap confidence intervals
- `results/role_stage_distribution.csv` - Role Ã— stage analysis

### Visualizations
- `analysis/overview_dashboard.png` - Key metrics overview
- `analysis/detailed_analysis.png` - Advanced analysis plots
- `analysis/results_explanation.md` - Detailed results explanation

### Artifacts
- `config_runtime.json` - Runtime configuration
- `prompt_one_shot.txt` - Classification prompt
- `codebook_min.json` - Stage definitions

## ğŸ”§ Configuration

### Environment Variables
```bash
export OPENAI_API_KEY="your-api-key"
export PILOT_LIMIT=100          # Subset size for testing
export MODEL_NAME="gpt-4"       # LLM model to use
export TEMPERATURE=0.1          # Sampling temperature
```

### Configuration Files
- `config/default.json` - Default settings
- `config/production.json` - Production settings
- `config/development.json` - Development settings

## ğŸ“ˆ Understanding Results

### Cognitive Presence Stages
1. **Triggering (Stage 1)** - Problem identification, questions
2. **Exploration (Stage 2)** - Information seeking, brainstorming
3. **Integration (Stage 3)** - Synthesizing ideas, explanations
4. **Resolution (Stage 4)** - Applying solutions, confirming results

### CP-Bench Metrics
- **SWS (Stage Weighted Score)**: Measures cognitive engagement level
- **PC (Progressive Coherence)**: Measures stage progression quality
- **RA (Resolution Attainment)**: Measures problem-solving success
- **CPI (Cognitive Presence Index)**: Overall cognitive presence score

### Interpretation Guidelines
- **CPI > 0.6**: Strong cognitive presence
- **CPI 0.3-0.6**: Moderate cognitive presence
- **CPI < 0.3**: Weak cognitive presence

## ğŸ§ª Advanced Usage

### Active Learning
```python
from src.active_learning import ActiveLearningSelector

# Find low-confidence samples for annotation
selector = ActiveLearningSelector()
samples = selector.select_low_confidence(classifications, n=50)
```

### Bias Analysis
```python
from src.evaluation import BiasAnalyzer

# Analyze role-based bias
analyzer = BiasAnalyzer()
bias_report = analyzer.analyze(classifications)
```

### Context Variants
```python
from src.classification import ContextVariantClassifier

# Test different context windows
classifier = ContextVariantClassifier()
results = classifier.classify_with_context(messages, context_size=5)
```

## ğŸ“š Documentation

- `docs/user_guide.md` - Detailed usage guide
- `docs/api_reference.md` - API documentation
- `docs/interpretation_guide.md` - Results interpretation
- `docs/development_guide.md` - Development guidelines

## ğŸ› Troubleshooting

### Common Issues

**1. OpenAI API Errors**
```bash
# Check API key
echo $OPENAI_API_KEY

# Test connection
python scripts/test_api.py
```

**2. Memory Issues**
```bash
# Reduce batch size
export BATCH_SIZE=10

# Use smaller subset
export PILOT_LIMIT=50
```

**3. Classification Failures**
```bash
# Check prompt formatting
python scripts/validate_prompt.py

# Test with sample data
python scripts/test_classification.py
```

### Debug Mode
```bash
# Enable debug logging
export LOG_LEVEL=DEBUG

# Run with verbose output
python scripts/run_pipeline.py --verbose
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- StudyChat dataset from HuggingFace
- CP-Bench metrics framework
- OpenAI GPT-4 for classification
- Community contributors

---

**For questions and support**: Open an issue or contact the maintainers.
